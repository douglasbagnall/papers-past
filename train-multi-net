#!/usr/bin/python
import argparse
import os
import sys
from alphabet import read_header
import charmodel
import random
import itertools
import json


def get_alphabet(books, ignore_case=False, collapse_space=False):
    texts = []
    for d in books.values():
        texts.extend(d.values())
    text = ''.join(texts)
    a = charmodel.Alphabet(text, ignore_case=ignore_case,
                           collapse_space=collapse_space,
                           threshold=3e-6)
    print a.alphabet
    print a.collapsed_chars
    return a


def get_net(alphabet, classnames, **kwargs):
    return charmodel.Net(alphabet, classnames, **kwargs)


def read_body(f):
    return ''.join(line for line in f)


def encode_books(alphabet, raw_books):
    books = {}
    for book, raw_chapters in raw_books.items():
        items = raw_chapters.items()
        random.shuffle(items)
        books[book] = itertools.cycle((k, alphabet.encode_text(v))
                                      for k, v in items)
    return books

def load_texts(dir, min_per_class):
    books = {}
    for fn in os.listdir(dir):
        ffn = os.path.join(dir, fn)
        f = open(ffn)
        header, n_bytes = read_header(f)
        text = read_body(f)
        f.close()
        chapter_id = header['id']
        try:
            book_id = chapter_id[:chapter_id.rindex('-')]
        except ValueError:
            book_id = chapter_id
        book_chapters = books.setdefault(book_id, {})
        book_chapters[chapter_id] = text

    if min_per_class is None:
        return books
    return {k: v for k, v in books.items()
            if len(v) > min_per_class}

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('-c', '--chapter-dir',
                        help="find training text here")
    parser.add_argument('-n', '--basename',
                        help="base filenames upon this")
    parser.add_argument('-M', '--min-chapters-per-class', type=int, metavar='<n>',
                        help="ignore classes with fewer than this many examples")
    parser.add_argument('--print-book-list', action='store_true',
                        help="print the list of classes found")
    parser.add_argument('-H', '--hidden-size', type=int, default=199, metavar='<nodes>',
                        help="number of hidden nodes")
    parser.add_argument('-r', '--rng-seed', type=int, default=-1,
                        help="rng seed (-1 for auto)")
    parser.add_argument('-e', '--sub-epochs', type=int, default=1,
                        help="how many cycles through the books to do")
    parser.add_argument('--batch-size', type=int, default=20, metavar='<int>',
                        help="mini-batch size")
    parser.add_argument('--presynaptic-noise', type=float, default=0, metavar='<float>',
                        help="Add this much presynaptic noise")
    parser.add_argument('-l', '--learn-rate', type=float, default=1e-3,
                        help=charmodel.Net.learn_rate.__doc__)
    parser.add_argument('-L', '--leakage', type=float, default=-1,
                        help=("how much training leaks into other classes "
                              "[0-1] or negative"))
    parser.add_argument('--leakage-decay', type=float, default=1,
                        help="change in leakage per sub-epoch")
    parser.add_argument('--learn-rate-decay', type=float, default=1,
                        help="change in learn-rate per sub-epoch")
    parser.add_argument('-m', '--momentum', type=float, default=0.95, metavar='<0-1>',
                        help=charmodel.Net.momentum.__doc__)
    parser.add_argument('--momentum-weight', type=float, default=0.5, metavar='<0-1>',
                        help=charmodel.Net.momentum_weight.__doc__)
    parser.add_argument('--log-file', default=None,
                        help="log to this file")
    parser.add_argument('-v', '--verbose', action='store_true',
                        help="print more to stderr")
    parser.add_argument('--enable-fp-exceptions', action='store_true',
                        help="crash on bad floating point errors")
    parser.add_argument('--temporal-pgm-dump', action='store_true',
                        help=("save images showing changing state "
                              "of input/error vectors"))
    parser.add_argument('--periodic-pgm-dump', metavar='"({ih,ho,bi}[wmdt] )*"',
                        help=("Periodically dump images of weights;"
                              "string determines which"))
    parser.add_argument('--periodic-pgm-period', type=int, default=1000,
                        help=("periodicity of periodic-pgm-dump"))
    parser.add_argument('--learning-method', type=int, default=4,
                        help=("0: weighted, 2: simplified N., "
                              "3: classical, 4: adagrad"))
    parser.add_argument('--activation', type=int, default=2,
                        help=("1: ReLU, 2: ReSQRT, 3: ReLOG, 4: "
                              "ReTANH, 5: clipped ReLU"))
    parser.add_argument('--collapse-space', action='store_true',
                        help="treat all runs of whitespace as a single space")
    parser.add_argument('--ignore-case', action='store_true',
                        help="treat capitals as lowercase")

    args = parser.parse_args()

    if args.enable_fp_exceptions:
        charmodel.enable_fp_exceptions()

    if args.rng_seed != -1:
        random.seed(args.rng_seed)

    raw_books = load_texts(args.chapter_dir, args.min_chapters_per_class)
    booknames = sorted(raw_books.keys())

    if args.print_book_list:
        for k in booknames:
            print k
        sys.exit()

    print args.log_file

    leakage = args.leakage

    alphabet = get_alphabet(raw_books, args.ignore_case, args.collapse_space)

    metadata = json.dumps({
        'alphabet': alphabet.alphabet,
        'collapse_chars': alphabet.collapsed_chars,
        'version': 1,
        'classnames': booknames,
        'case_insensitive': args.ignore_case,
        'utf8': True,
        'collapse_space': args.collapse_space,
        'batch_size': args.batch_size,
        'verbose': args.verbose,
        'momentum': args.momentum,
        'learning_method': args.learning_method,
        'temporal_pgm_dump': args.temporal_pgm_dump,
        'periodic_pgm_dump': args.periodic_pgm_dump,
        'periodic_pgm_period': args.periodic_pgm_period,
        'basename': args.basename,
    }, sort_keys=True)

    net_kwargs = {}
    for k, v in vars(args).items():
        if k in ("bptt_depth",
                "hidden_size",
                 "rng_seed",
                 "log_file",
                 "verbose",
                 "learn_rate",
                 "temporal_pgm_dump",
                 "periodic_pgm_dump",
                 "periodic_pgm_period",
                 "basename",
                 "activation",
                 "learning_method",
                 "batch_size",
                 "filename"):
            net_kwargs[k] = v


    net = get_net(alphabet, booknames, metadata=metadata,
                  **net_kwargs)

    net.batch_size = args.batch_size

    books = encode_books(alphabet, raw_books)

    for i in range(args.sub_epochs):
        print "doing sub-epoch %d with learn-rate %s, leakage %s" % (i,
                                                                     net.learn_rate,
                                                                     leakage)
        for book_id, chapters in books.items():
            if args.verbose and False:
                net.dump_parameters()
            cid, c = chapters.next()
            net.train(c, book_id, leakage=leakage)

        net.save()
        leakage *= args.leakage_decay
        net.learn_rate *= args.learn_rate_decay

main()
